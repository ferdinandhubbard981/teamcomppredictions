{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "bayesian optimization for teamcompanalyzer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S17sgeVVTtP",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Jhqv5CVTu_",
        "colab_type": "text"
      },
      "source": [
        "# Part 8.4: Bayesian Hyperparameter Optimization for Keras\n",
        "\n",
        "Bayesian Hyperparameter Optimization is a method of finding hyperparameters in a more efficient way than a grid search.  Because each candidate set of hyperparameters requires a retraining of the neural network, it is best to keep the number of candidate sets to a minimum. Bayesian Hyperparameter Optimization achieves this by training a model to predict good candidate sets of hyperparameters.\n",
        "\n",
        "Snoek, J., Larochelle, H., & Adams, R. P. (2012). [Practical bayesian optimization of machine learning algorithms](https://arxiv.org/pdf/1206.2944.pdf). In *Advances in neural information processing systems* (pp. 2951-2959).\n",
        "\n",
        "\n",
        "* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n",
        "* [hyperopt](https://github.com/hyperopt/hyperopt)\n",
        "* [spearmint](https://github.com/JasperSnoek/spearmint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWeTYCwglatg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "83c59ec2-0d0d-42e9-ff52-6fb0bd61adab"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Sep 12 07:36:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUmPhl-VcMaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samplesizename = \"100k\"\n",
        "epochcount = 30\n",
        "logsfilename = samplesizename + str(epochcount) + \"epoch\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFxQ1X8vjbaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPF8d8tCppte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "514344ff-0119-4479-d8c4-9511534f2d52"
      },
      "source": [
        "def getdata():\n",
        "  #get data\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  %cp \"/content/drive/My Drive/teamcompanalyzer/100k.rar\" /content/data.rar\n",
        "  %rm data.json\n",
        "  %rm labels.json\n",
        "  !unrar x data.rar\n",
        "  x = []\n",
        "  y = []\n",
        "  with open(\"data.json\", \"r\") as f:\n",
        "    x = np.array(json.load(f))\n",
        "\n",
        "  with open(\"labels.json\", \"r\") as f:\n",
        "    y = np.array(json.load(f))\n",
        "  return x, y\n",
        "x, y = getdata()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "rm: cannot remove 'data.json': No such file or directory\n",
            "rm: cannot remove 'labels.json': No such file or directory\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from data.rar\n",
            "\n",
            "Extracting  data.json                                                    \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  labels.json                                                  \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcCFaCSVTv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "import tensorflow.keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def generate_model(dropout, neuronPct, neuronShrink):\n",
        "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
        "    neuronCount = int(neuronPct * 5000)\n",
        "    \n",
        "    # Construct neural network\n",
        "    # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n",
        "    model = Sequential()\n",
        "\n",
        "    # So long as there would have been at least 25 neurons and fewer than 10\n",
        "    # layers, create a new layer.\n",
        "    layer = 0\n",
        "    while neuronCount>25 and layer<10:\n",
        "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
        "        if layer==0:\n",
        "            model.add(Dense(neuronCount,\n",
        "                #input_dim=x.shape[1] \n",
        "                input_shape=(1500,), \n",
        "                activation=PReLU()))\n",
        "                #activation=\"relu\"))\n",
        "        else:\n",
        "            model.add(Dense(neuronCount, activation=PReLU())) \n",
        "        layer += 1\n",
        "\n",
        "        # Add dropout after each hidden layer\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        # Shrink neuron count for each layer\n",
        "        neuronCount = neuronCount * neuronShrink\n",
        "\n",
        "    #model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlvCslbcVTwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "9952f997-4d45-46bf-d8ed-0de9e5c08133"
      },
      "source": [
        "# Generate a model and see what the resulting structure looks like.\n",
        "model = generate_model(dropout=0.2, neuronPct=0.1, neuronShrink=0.25)\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               751000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 125)               62750     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 125)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 31)                3937      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 31)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 32        \n",
            "=================================================================\n",
            "Total params: 817,719\n",
            "Trainable params: 817,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmpk2VV8VTxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n",
        "    SPLITS = 2\n",
        "\n",
        "    # Bootstrap\n",
        "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
        "\n",
        "    # Track progress\n",
        "    mean_benchmark = []\n",
        "    epochs_needed = []\n",
        "    num = 0\n",
        "    \n",
        "\n",
        "    # Loop through samples\n",
        "    for train, test in boot.split(x, y):\n",
        "        start_time = time.time()\n",
        "        num+=1\n",
        "\n",
        "        # Split train and test\n",
        "        x_train = x[train]\n",
        "        y_train = y[train]\n",
        "        x_test = x[test]\n",
        "        y_test = y[test]\n",
        "\n",
        "        model = generate_model(dropout, neuronPct, neuronShrink)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr), metrics=[\"accuracy\"]) #changed\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=100, verbose=2, mode='auto', restore_best_weights=True)\n",
        "\n",
        "        # Train on the bootstrap sample\n",
        "        model.fit(x_train,y_train, #batch_size = 16, \n",
        "                  validation_data=(x_test,y_test),\n",
        "                  callbacks=[monitor],verbose=0,epochs=epochcount)\n",
        "        epochs = monitor.stopped_epoch\n",
        "        epochs_needed.append(epochs)\n",
        "\n",
        "        # Predict on the out of boot (validation)\n",
        "        pred = model.predict(x_test)\n",
        "        #print(pred)#\n",
        "        # Measure this bootstrap's log loss\n",
        "        y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
        "        #print(\"compare\" + str(y_compare))\n",
        "        #print(\"test\" + str(y_test))\n",
        "        try:\n",
        "            score = metrics.log_loss(y_test, pred, eps=1e-7) #used to be y_compare\n",
        "        except ValueError:\n",
        "            return -100.0\n",
        "        #print(score)\n",
        "        mean_benchmark.append(score)\n",
        "        m1 = statistics.mean(mean_benchmark)\n",
        "        m2 = statistics.mean(epochs_needed)\n",
        "        mdev = statistics.pstdev(mean_benchmark)\n",
        "\n",
        "        # Record this iteration\n",
        "        time_took = time.time() - start_time\n",
        "        \n",
        "    tensorflow.keras.backend.clear_session()\n",
        "    return (-m1)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7ljU5d4Vo-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(evaluate_network(\n",
        "#    dropout=0.2,\n",
        "#    lr=1e-3,\n",
        "#    neuronPct=0.2,\n",
        "#    neuronShrink=0.2))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOybT-nrVTx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "05799987-7c2a-46c0-e2d5-8da7552675c8"
      },
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "import time\n",
        "from bayes_opt.logger import JSONLogger\n",
        "from bayes_opt.event import Events\n",
        "from bayes_opt.util import load_logs\n",
        "\n",
        "class newJSONLogger(JSONLogger) :\n",
        "\n",
        "      def __init__(self, path):\n",
        "            self._path=None\n",
        "            super(JSONLogger, self).__init__()\n",
        "            self._path = path if path[-5:] == \".json\" else path + \".json\"\n",
        "# Supress NaN warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'dropout': (0.0, 0.499),\n",
        "           'lr': (0.0, 0.1),\n",
        "           'neuronPct': (0.01, 1),\n",
        "           'neuronShrink': (0.01, 1)\n",
        "          }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_network,\n",
        "    pbounds=pbounds,\n",
        "    verbose=1,  # verbose = 1 prints only when a maximum \n",
        "    # is observed, verbose = 0 is silent\n",
        "    random_state=1,\n",
        ")\n",
        "try:\n",
        "  load_logs(optimizer, logs=[\"/content/drive/My Drive/teamcompanalyzer/\" + logsfilename + \".json\"]) #to load a previous save\n",
        "except FileNotFoundError:\n",
        "  pass\n",
        "\n",
        "logger = newJSONLogger(\"/content/drive/My Drive/teamcompanalyzer/\" + logsfilename + \".json\") # to save progress\n",
        "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "\n",
        "#ScreenLogger(verbose=2)\n",
        "#optimizer.subscribe()\n",
        "start_time = time.time()\n",
        "optimizer.maximize(init_points=10, n_iter=1000)\n",
        "time_took = time.time() - start_time\n",
        "\n",
        "print(\"Total runtime:\" + str(time_took))\n",
        "print(optimizer.max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.16.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp36-none-any.whl size=11685 sha256=84bf81ce6a1af6bdd30c9af65cd7f6b5a0afe4d1aa84a68604a4eec61abaf7c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClXguZOjVTyN",
        "colab_type": "text"
      },
      "source": [
        "Total runtime:4567.445779800415\n",
        "\n",
        "{'target': -0.6932720859646797, 'params': {'dropout': 0.24858337004439982, 'lr': 0.004121493053087515, 'neuronPct': 0.33485495615382616, 'neuronShrink': 0.6423925330540974}}\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    }
  ]
}